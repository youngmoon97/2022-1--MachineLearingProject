# -*- coding: utf-8 -*-
"""maker.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1djexcKv6DL5CC7ysWBD5OLt0PJLjdxGz
"""

import numpy as np
import os

os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'

# 훈련 시킬 데이터 삽입하는 구문
# 손 제스처 총 8가지 데이터를 삽입 후 하나로 합쳐준다.

actions = [
    'good',
    'bad',
    'ok',
    'love',
    'call',
    'hello',
    'no',
    'come'
]

data = np.concatenate([
    np.load('/content/seq_good_1654171736.npy'),
    np.load('/content/seq_bad_1654171736.npy'),
    np.load('/content/seq_ok_1654171736.npy'),
    np.load('/content/seq_love_1654171736.npy'),
    np.load('/content/seq_call_1654171736.npy'),
    np.load('/content/seq_hello_1654171736.npy'),
    np.load('/content/seq_no_1654171736.npy'),
    np.load('/content/seq_come_1654171736.npy')

], axis=0)

data.shape

# (데이터셋의 개수, 윈도우 사이즈, 한 윈도우당 데이터의 개수)

# 마지막 값을 빼고 x 데이터를 만든다. 그 마지막 값만 label로 만들어준다.
x_data = data[:, :, :-1]
labels = data[:, 0, -1]

print(x_data.shape)
print(labels.shape)

# 99개로 줄여든 것을 볼 수 있다.
# 그리고 4275개만 따로 뽑힌 것을 볼 수 있다.
# 이 라벨을 one-hot encoding을 시킨다.
# one-hot encoding : 각 카테고리를 하나의 새로운 열로 만든다.

from tensorflow.keras.utils import to_categorical # tensorflow의 to_categorical을 사용한다.

y_data = to_categorical(labels, num_classes=len(actions))
y_data.shape

# (총 데이터 개수, 학습 시킬 데이터 개수)

from sklearn.model_selection import train_test_split # sklearn의 train_test_split을 사용한다.
                                                      # train_test_split : train set과 test set을 나눈다.
x_data = x_data.astype(np.float32)
y_data = y_data.astype(np.float32)

# train set은 90% test set은 10% 사용한다.
x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=2021)

print(x_train.shape, y_train.shape)
print(x_val.shape, y_val.shape)

# Sequential API 사용. LSTM, Dense 두 개를 연결.
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

model = Sequential([
    LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),  # LSTM 노드 갯수 64개. input_shape (train_shape) 윈도우의 크기와 x, y, z, visibility 각도.
     
    # LSTM 결과를 Dense에 삽입.
    Dense(32, activation='relu'), # Dense의 노드 갯수는 32개.
    Dense(len(actions), activation='softmax') # 액션의 갯수.
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc']) # categorical_crossetropy : 8개의 액션중에 어떤 것인지 모델에게 추론하도록 한다.
model.summary()

# total params에 있는 갯수가 학습을 시킬 데이터 수다.

from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau

history = model.fit(
    x_train,
    y_train,
    validation_data=(x_val, y_val),
    epochs=200, # 200번 학습을 시킨다.
    callbacks=[
        ModelCheckpoint('models/model.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'), # 모델을 저장하는 구문
        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')
    ]
)

# 200번 학습을 시킬 때 가장 높은 값이 나온 model.h5에 저장한다.

import matplotlib.pyplot as plt

fig, loss_ax = plt.subplots(figsize=(16, 10))
acc_ax = loss_ax.twinx()

loss_ax.plot(history.history['loss'], 'y', label='train loss')
loss_ax.plot(history.history['val_loss'], 'r', label='val loss')
loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
loss_ax.legend(loc='upper left')

acc_ax.plot(history.history['acc'], 'b', label='train acc')
acc_ax.plot(history.history['val_acc'], 'g', label='val acc')
acc_ax.set_ylabel('accuracy')
acc_ax.legend(loc='upper left')

plt.show()

# 학습이 완료되면 그래프를 보여준다.